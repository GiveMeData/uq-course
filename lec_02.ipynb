{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 2 - Introduction to Probability Theory\n",
    "\n",
    "> Probability theory is nothing but common sense reduced to calculation. P. Laplace (1812)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "+ To use probability theory to represent states of knowledge.\n",
    "+ To use probability theory to extend Aristotelian logic to reason under uncertainty.\n",
    "+ To learn about the **pruduct rule** of probability theory.\n",
    "+ To learn about the **sum rule** of probability theory.\n",
    "+ What is a **random variable**?\n",
    "+ What is a **discrete random variable**?\n",
    "+ When are two random variable **independent**?\n",
    "+ What is a **continuous random variable**?\n",
    "+ What is the **cumulative distribution function**?\n",
    "+ What is the **probability density function**?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Readings\n",
    "\n",
    "Before coming to class, please read the following:\n",
    "\n",
    "+ [Chapter 1 of Probabilistic Programming and Bayesian Methods for Hackers](http://nbviewer.ipython.org/github/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/blob/master/Chapter1_Introduction/Chapter1.ipynb)\n",
    "+ [Chapter 1](http://home.fnal.gov/~paterno/images/jaynesbook/cc01p.pdf) of \\cite{jaynes2003}.\n",
    "+ [Chapter 2](http://home.fnal.gov/~paterno/images/jaynesbook/cc02p.pdf) of \\cite{jaynes2003} (skim through)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The basic desiderata of probability theory\n",
    "It is actually possible to derive the rules of probability based on a system of common sense requirements.\n",
    "Paraphrasing \n",
    "[Chapter 1](http://home.fnal.gov/~paterno/images/jaynesbook/cc01p.pdf) of \\cite{jaynes2003}),\n",
    "we would like our system to satisfy the following desiderata:\n",
    "\n",
    "1) *Degrees of plausibility are represented by real numbers.*\n",
    "\n",
    "2) *The system should have a qualitative correspondance with common sense.*\n",
    "\n",
    "3) *The system should be consistent in the sense that:*\n",
    "    \n",
    "   + *If a conclusion can be reasoned out in more than one way, then every possible way must lead to the same result.*\n",
    "    \n",
    "   + *All the evidence relevant to a question should be taken into account.*\n",
    "    \n",
    "   + *Equivalent states of knowledge must be represented by equivalent plausibility assignments.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to speak about probabilities?\n",
    "Let\n",
    "+ A be a logical sentence,\n",
    "+ B be another logical sentence, and\n",
    "+ and I be all other information we know.\n",
    "\n",
    "There is no restriction on what A and B may be as soon as none of them is a contradiction.\n",
    "We write as a shortcut:\n",
    "$$\n",
    "\\mbox{not A} \\equiv \\neg,\n",
    "$$\n",
    "$$\n",
    "A\\;\\mbox{and}\\;B \\equiv A,B \\equiv AB,\n",
    "$$\n",
    "$$\n",
    "A\\;\\mbox{or}\\;B \\equiv A + B.\n",
    "$$\n",
    "\n",
    "We **write**:\n",
    "$$\n",
    "p(A|BI),\n",
    "$$\n",
    "and we **read**:\n",
    "> the probability of A being true given that we know that B and I is true\n",
    "\n",
    "or (assuming knowledge I is implied)\n",
    "\n",
    "> the probability of A being true given that we know that B is true\n",
    "\n",
    "or (making it even shorter)\n",
    "\n",
    "> the probability of A given B.\n",
    "\n",
    "$$\n",
    "p(\\mbox{something} | \\mbox{everything known}) = \\mbox{probability samething is true conditioned on what is known}.\n",
    "$$\n",
    "\n",
    "$p(A|B,I)$ is just a number between 0 and 1 that corresponds to the degree of plaussibility of A conditioned on B and I.\n",
    "0 and 1 are special.\n",
    "\n",
    "+ If\n",
    "$$\n",
    "p(A|BI) = 0,\n",
    "$$\n",
    "we say that we are certain that A is false if B is true.\n",
    "\n",
    "+ If\n",
    "$$\n",
    "p(A|BI) = 1,\n",
    "$$\n",
    "we say that we are certain that A is false if B is false.\n",
    "\n",
    "+ If\n",
    "$$\n",
    "p(A|BI) \\in (0, 1),\n",
    "$$\n",
    "we say that we are uncertain about A given that B is false.\n",
    "Depending on whether $p(A|B,I)$ is closer to 0 or 1 we beleive more on one possibiliy or another.\n",
    "Complete ignorance corresponds to a probability of 0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The rules of probability theory\n",
    "\n",
    "According to\n",
    "[Chapter 2](http://home.fnal.gov/~paterno/images/jaynesbook/cc02m.pdf) of \\cite{jaynes2003} the desiderata are enough\n",
    "to derive the rules of probability.\n",
    "These rules are:\n",
    "\n",
    "+ The **obvious rule** (in lack of a better name):\n",
    "$$\n",
    "p(A | I) + p(\\neg A | I) = 1.\n",
    "$$\n",
    "\n",
    "+ The **product rule** (also known as the Bayes rule or Bayes theorem):\n",
    "$$\n",
    "p(AB|I) = p(A|BI)p(B|I).\n",
    "$$\n",
    "or\n",
    "$$\n",
    "p(AB|I) = p(B|AI)p(A|I).\n",
    "$$\n",
    "\n",
    "These two rules are enough to compute any probability we want. Let us demonstrate this by a very simple example.\n",
    "\n",
    "### Example: Drawing balls from a box without replacement\n",
    "Consider the following example of prior information I:\n",
    "\n",
    "> We are given a box with 10 balls 6 of which are red and 4 of which are blue.\n",
    "The box is sufficiently mixed so that so that when we get a ball from it, we don't know which one we pick.\n",
    "When we take a ball out of the box, we do not put it back.\n",
    "\n",
    "Let A be the sentence:\n",
    "\n",
    "> The first ball we draw is blue.\n",
    "\n",
    "Intuitively, we would set the probability of A equal to:\n",
    "$$\n",
    "p(A|I) = \\frac{4}{10}.\n",
    "$$\n",
    "\n",
    "This choice can actually be justified, but we will come to this later in this course.\n",
    "From the \"obvious rule\", we get that the probability of not drawing a blue ball, i.e.,\n",
    "the probability of drawing a red ball in the first draw is:\n",
    "$$\n",
    "p(\\neg A|I) = 1 - p(A|I) = 1 - \\frac{4}{10} = \\frac{6}{10}.\n",
    "$$\n",
    "\n",
    "Now, let B be the sentence:\n",
    "\n",
    "> The second ball we draw is red.\n",
    "\n",
    "What is the probability that we draw a red ball in the second draw given that we drew a blue ball in the first draw?\n",
    "Just before our second draw, there remain 9 bals in the box, 3 of which are blue and 6 of which are red.\n",
    "Therefore:\n",
    "$$\n",
    "p(B|AI) = \\frac{6}{9}.\n",
    "$$\n",
    "\n",
    "We have not used the product rule just yet. What if we wanted to find the probability that we draw a blue during the first draw and a red during the second draw? Then,\n",
    "$$\n",
    "p(AB|I) = p(A|I)p(B|AI) = \\frac{4}{10}\\frac{6}{9} = \\frac{24}{90}.\n",
    "$$\n",
    "\n",
    "What about the probability o a red followed by a blue? Then,\n",
    "$$\n",
    "p(\\neg AB|I) = p(\\neg A|I)p(B|AI) = \\left[1 - p(A|I) \\right]p(B|\\neg AI) = \\frac{6}{10}\\frac{5}{9} = \\frac{30}{90}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other rules of probability theory\n",
    "\n",
    "All the other rules of probability theory can be derived from these two rules.\n",
    "To demonstrate this, let's prove that:\n",
    "$$\n",
    "p(A + B|I) = p(A|I) + p(B|I) - p(AB|I).\n",
    "$$\n",
    "Here we go:\n",
    "\\begin{eqnarray*}\n",
    "p(A+B|I) &=& 1 - p(\\neg A \\neg B|I)\\;\\mbox{(obvious rule)}\\\\\n",
    "         &=& 1 - p(\\neg A|\\neg BI)p(\\neg B|I)\\;\\mbox{(product rule)}\\\\\n",
    "         &=& 1 - [1 - p(A |\\neg BI)]p(\\neg B|I)\\;\\mbox{(obvious rule)}\\\\\n",
    "         &=& 1 - p(\\neg B|I) + p(A|\\neg B I)p(\\neg B|I)\\\\\n",
    "         &=& 1 - [1 - p(B|I)] + p(A|\\neg B I)p(\\neg B|I)\\\\\n",
    "         &=& p(B|I) + p(A|\\neg B I)p(\\neg B|I)\\\\\n",
    "         &=& p(B|I) + p(A\\neg B|I)\\\\\n",
    "         &=& p(B|I) + p(\\neg B|AI)p(A|I)\\\\\n",
    "         &=& p(B|I) + [1 - p(B|AI)] p(A|I)\\\\\n",
    "         &=& p(B|I) + p(A|I) - p(B|AI)p(A|I)\\\\\n",
    "         &=& p(A|I) + p(B|I) - p(AB|I).\n",
    "\\end{eqnarray*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The sum rule\n",
    "Now consider a finite set of logical sentences, $B_1,\\dots,B_n$ such that:\n",
    "1. One of them is definitely true:\n",
    "    $$\n",
    "    p(B_1+\\dots+B_n|I) = 1.\n",
    "    $$\n",
    "2. They are mutually exclusive:\n",
    "    $$\n",
    "    p(B_iB_j|I) = 0,\\;\\mbox{if}\\;i\\not=j.\n",
    "    $$\n",
    "\n",
    "The **sum rule** states that:\n",
    "    $$\n",
    "    P(A|I) = \\sum_i p(AB_i|I) = \\sum_i p(A|B_i I)p(B_i|I).\n",
    "    $$\n",
    "We can prove this by induction, but let's just prove it for $n=2$:\n",
    "\\begin{eqnarray*}\n",
    "p(A|I) &=& p[A(B_1+B_2|I]\\\\\n",
    "       &=& p(AB_1 + AB_2|I)\\\\\n",
    "       &=& p(AB_1|I) + p(AB_2|I) - p(AB_1B_2|I)\\\\\n",
    "       &=& p(AB_1|I) + p(AB_2|I),\n",
    "\\end{eqnarray*}\n",
    "since\n",
    "$$\n",
    "p(AB_1B_2|I) = p(A|B_1B_2|I)p(B_1B_2|I) = 0.\n",
    "$$\n",
    "\n",
    "Let's go back to our example. We can use the sum rule to compute the probability of getting a red ball on the second draw independently of what we drew first. This is how it goes:\n",
    "\\begin{eqnarray*}\n",
    "p(B|I) &=& p(AB|I) + p(\\neg AB|I)\\\\\n",
    "       &=& p(B|AI)p(A|I) + p(B|\\neg AI) p(\\neg A|I)\\\\\n",
    "       &=& \\frac{6}{9}\\frac{4}{10} + \\frac{5}{9}\\frac{6}{10}\\\\\n",
    "       &=& \\dots\n",
    "\\end{eqnarray*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Variables\n",
    "\n",
    "The formal mathematical definition of a random variable involves measure theory and is well beyond the scope of this course.\n",
    "Fortunately, we do not have to go through that route to get a theory that is useful in applications.\n",
    "For us, a **random variable** $X$ will just be a variable of our problem whose value is unknown to us.\n",
    "Note that, you should not take the word \"random\" too literally.\n",
    "If we could, we would change the name to **uncertain** or **unknown** variable.\n",
    "A random variable could correspond to something fixed but unknown, e.g., the number of balls in a box,\n",
    "or it could correspond to something truely random, e.g., the number of particles that hit a [Geiger counter](https://en.wikipedia.org/wiki/Geiger_counter) in a specific time interval.\n",
    "\n",
    "### Discrete Random Variables\n",
    "We say that a random variable $X$ is discrete if the possible values it can take are discrete (possibly countably infinite).\n",
    "We write:\n",
    "$$\n",
    "p(X = x|I) \n",
    "$$\n",
    "and we read \"the probability of $X$ being $x$\".\n",
    "If it does not cause any ambiguity, sometimes we will simplify the notation to:\n",
    "$$\n",
    "p(x) \\equiv p(X=x|I).\n",
    "$$\n",
    "Note that $p(X=x)$ is actually a discrete function of $x$ which depends on our beliefs about $X$.\n",
    "The function $p(x) = p(X=x|I)$ is known as the probability density function of $X$.\n",
    "\n",
    "Now let $Y$ be another random variable. \n",
    "The **sum rule** becomes:\n",
    "$$\n",
    "p(X=x|I) = \\sum_{y}p(X=x,Y=y|I) = \\sum_y p(X=x|Y=y,I)p(Y=y|I),\n",
    "$$\n",
    "or in simpler notation:\n",
    "$$\n",
    "p(x) = \\sum_y p(x,y) = \\sum_y p(x|y)p(y).\n",
    "$$\n",
    "The function $p(X=x, Y=y|I) \\equiv p(x, y)$ is known as the joint *probability mass function* of $X$ and $Y$.\n",
    "\n",
    "The **product rule** becomes:\n",
    "$$\n",
    "p(X=x,Y=y|I) = p(X=x|Y=y,I)p(Y=y|I),\n",
    "$$\n",
    "or in simpler notation:\n",
    "$$\n",
    "p(x,y) = p(x|y)p(y).\n",
    "$$\n",
    "\n",
    "We say that $X$ and $Y$ are **independent** and write:\n",
    "$$\n",
    "X\\perp Y|I,\n",
    "$$\n",
    "if knowledge of one does not yield any information about the other.\n",
    "Mathematically, $Y$ gives no information about $X$ if:\n",
    "$$\n",
    "p(x|y) = p(x).\n",
    "$$\n",
    "From the product rule, however, we get that:\n",
    "$$\n",
    "p(x) = p(x|y) = \\frac{p(x,y)}{p(y)},\n",
    "$$\n",
    "from which we see that the joint distribution of $X$ and $Y$ must factorize as:\n",
    "$$\n",
    "p(x, y) = p(x) p(y).\n",
    "$$\n",
    "It is trivial to show that if this factorization holds, then\n",
    "$$\n",
    "p(y|x) = p(y),\n",
    "$$\n",
    "and thus $X$ yields no information about $Y$ either."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous Random Variables\n",
    "A random variable $X$ is continuous if the possible values it can take are continuous. The probability of a continuous variable getting a specific value is always zero. Therefore, we cannot work directly with probability mass functions as we did for discrete random variables. We would have to introduce the concepts of the **cumulative distribution function** and the **probability density function**. Fortunately, with the right choice of mathematical symbols, the theory will look exactly the same.\n",
    "\n",
    "Let us start with a real continuous random variable $X$, i.e., a random variable taking values in the real line $\\mathbb{R}$. Let $x \\in\\mathbb{R}$ and consider the probability of $X$ being less than or equal to $x$:\n",
    "$$\n",
    "F(x) := p(X\\le x|I).\n",
    "$$\n",
    "$F(x)$ is known as the **cumulative distribution function** (CDF). Here are some properties of the CDF whose proof is\n",
    "left as an excersise:\n",
    "\n",
    "+ The CDF starts at zero and goes up to one:\n",
    "\\begin{equation}\n",
    "\\label{eq:CDF_bounds}\n",
    "F(-\\infty) = 0\\;\\mbox{and}\\;F(+\\infty) = 1.\n",
    "\\end{equation}\n",
    "\n",
    "+ $F(x)$ is an increasing function of $x$, i.e.,\n",
    "\\begin{equation}\n",
    "\\label{eq:CDF_monotone}\n",
    "x_1 \\le x_2 \\implies F(x_1)\\le F(x_2).\n",
    "\\end{equation}\n",
    "\n",
    "+ The probability of $X$ being in the interval $[x_1,x_2]$ is:\n",
    "\\begin{equation}\n",
    "\\label{eq:CDF_prob}\n",
    "p(x_1 \\le X \\le x_2|I) = F(x_2) - F(x_1).\n",
    "\\end{equation}\n",
    "\n",
    "Now, assume that the derivative of $F(x)$ with respect to $x$ exists.\n",
    "Let us call it $f(x)$:\n",
    "$$\n",
    "f(x) = \\frac{dF(x)}{dx}.\n",
    "$$\n",
    "Using the fundamental theorem of calculus, it is trivial to show Eq. (\\ref{eq:CDF_prob}) implies:\n",
    "\\begin{equation}\n",
    "p(x_1 \\le X \\le x_2|I) = \\int_{x_1}^{x_2}f(x)dx.\n",
    "\\end{equation}\n",
    "$f(x)$ is known as the **probability density function** (PDF) and it is measured in probability per unit of $X$.\n",
    "To see this note that:\n",
    "$$\n",
    "p(x \\le X \\le x + \\delta x|I) = \\int_{x}^{x+\\delta x}f(x')dx' \\approx f(x)\\delta x,\n",
    "$$\n",
    "so that:\n",
    "$$\n",
    "f(x) \\approx \\frac{p(x \\le X \\le x + \\delta x|I)}{\\delta x}.\n",
    "$$\n",
    "\n",
    "The PDF should satisfy the following properties:\n",
    "\n",
    "+ It should be positive\n",
    "$$\n",
    "f(x) \\ge 0,\n",
    "$$\n",
    "+ It should integrate to one:\n",
    "$$\n",
    "\\int_{-\\infty}^{\\infty} f(x) dx = 1.\n",
    "$$\n",
    "\n",
    "#### Notation about the PDF of continuous random variables\n",
    "In order to make all the formulas of probability theory the same, we define for a continuous random variable $X$:\n",
    "$$\n",
    "p(x) := f(x) = \\frac{dF(x)}{dx} = \\frac{d}{dx}p(X \\le x|I).\n",
    "$$\n",
    "But keep in mind, that if $X$ is continuous $p(x)$ is not a probability but a probability density.\n",
    "That is, it needs a $dx$ to become a probability.\n",
    "\n",
    "Let the PDF $p(x)$ of $X$ and the PDF $p(y)$ of $Y$ ($Y$ is another continuous random variable).\n",
    "We can find the PDF of the random variable $X$ conditioned on $Y$, i.e., the PDF of $X$ if $Y$ is directly observed.\n",
    "This is the **product rule** for continuous random variables:\n",
    "\\begin{equation}\n",
    "\\label{eq:continuous_bayes}\n",
    "p(y|x) = \\frac{p(x, y)}{p(y)},\n",
    "\\end{equation}\n",
    "where $p(x,y)$ is the **joint PDF** of $X$ and $Y$.\n",
    "The **sum rule** for continous random variables is:\n",
    "\\begin{equation}\n",
    "\\label{eq:continuous_sum}\n",
    "p(x) = \\int p(x, y) dy = \\int p(x | y) p(y) dy.\n",
    "\\end{equation}\n",
    "\n",
    "The similarity between these rules and the discrete ones is obvious.\n",
    "We have prepared a table to help you remember it.\n",
    "\n",
    "| Concept | Discrete Random Variables | Continuous Random Variables |\n",
    "|---|---------------|-----------------|\n",
    "|$p(x)$| in units of robability | in units of probability per unit of $X$|\n",
    "|sum rule| $\\sum_y p(x,y) = \\sum_y p(x|y)p(y)$ | $\\int_y p(x,y) dy = \\int_y p(x|y) p(y)$| \n",
    "|product rule| $p(x,y) = p(x|y)p(y)$ | $p(x,y) = p(x|y)p(y)$|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expectations\n",
    "\n",
    "Let $X$ be a random variable. The expectation of $X$ is defined to be:\n",
    "$$\n",
    "\\mathbb{E}[X] := \\mathbb{E}[X | I] = \\int x p(x) dx.\n",
    "$$\n",
    "Now let $g(x)$ be any function. The expectation of $g(X)$, i.e., the random variable defined after passing $X$ through $g(\\cdot)$, is:\n",
    "$$\n",
    "\\mathbb{E}[g(X)] := \\mathbb{E}[g(X)|I] = \\int g(x)p(x)dx.\n",
    "$$\n",
    "As usual, calling $\\mathbb{E}[\\cdot]$ is not a very good name.\n",
    "You may think of $\\mathbb{E}[g(X)]$ as the expected value of $g(X)$, but do not take it too far.\n",
    "Can you think of an example in which the expected value is never actually observed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional Expectation\n",
    "Let $X$ and $Y$ be two random variables. The conditional expectation of $X$ given $Y=y$ is defined to be:\n",
    "$$\n",
    "\\mathbb{E}[X|Y=y] := \\mathbb{E}[X|Y=y,I] = \\int xp(x|y)dx.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excersises\n",
    "\n",
    "To solve the following excercises use:\n",
    "+ Common sense\n",
    "+ The obvious, the product, and the sum rules of probability.\n",
    "\n",
    "\n",
    "1. This exercise demonstrates the probability theory is actually an extension of logic. Assume that you know that \"A implies B\". That is, your prior information is:\n",
    "   $$\n",
    "   I = \\{A\\implies B\\}.\n",
    "   $$\n",
    "   Show that:\n",
    "     1. $p(AB|I) = p(A|I)$ (use common sense).\n",
    "     2. If $p(A|I) = 1$, then $p(B|I) = 1$.\n",
    "     3. If $p(B|I) = 0$, then $p(A|I) = 0$.\n",
    "     4. B and C show that probability theory is consistent with Aristotelian logic. Now, you will discover how it extends it. Show that if B is true, then A becomes more plausible, i.e.\n",
    "      $$\n",
    "      p(A|BI) \\ge p(A|I).\n",
    "      $$\n",
    "     5. Give at least two examples of D that apply to various scientific fields. To get you started, here are two examples:\n",
    "       1. A: It is raining. B: There are clouds in the sky. Clearly, $A\\implies B$. D tells us that if there are clouds in the sky, raining becomes more plausible.\n",
    "       2. A: General  relativity. B: Light is deflected in the presence of massive bodies. Here $A\\implies B$. Observing that B is true makes A more plausible.\n",
    "     6. Show that if A is false, then B becomes less plausible.\n",
    "        $$\n",
    "        P(B|\\neg A I) \\le p(B|I).\n",
    "        $$\n",
    "     7. Can you think of an example of scientific reasoning that involves F? For example:\n",
    "        1. A: It is raining. B: There are clouds in the sky. F tells us that if it is not raining, then it is less plausible that there are clouds in the sky.\n",
    "     7. Do D and F contradict Karl Popper's *principle of falsification*, \"A theory in the empirical sciences can never be proven, but it can be falsified, meaning that it can and should be scrutinized by decisive experiments.\" Source: [Wikipedia](https://en.wikipedia.org/wiki/Karl_Popper))\n",
    "     \n",
    "2. Prove Eq. (\\ref{eq:CDF_bounds}) using only the basic rules.\n",
    "\n",
    "3. Prove Eq. (\\ref{eq:CDF_monotone}) using only the basic rules.\n",
    "\n",
    "4. Prove Eq. (\\ref{eq:CDF_prob}) using only the basic rules.\n",
    "\n",
    "5. Consider a coin fliping experiment in which you do not know if the coin is fair or not. That is, the probability of heads, say $X$, is a continuous random variable taking values between zero and one. Assign to it a uniform probability density, $p(x) = 1$. What is the corresponding CDF?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "(<a id=\"cit-jaynes2003\" href=\"#call-jaynes2003\">Jaynes, 2003</a>) E T Jaynes, ``_Probability Theory: The Logic of Science_'',  2003.  [online](http://bayes.wustl.edu/etj/prob/book.pdf)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
